<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Train SwinUnetr Pixel Embedding Network | Capture knowledge meta catalog</title>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css><script src=https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js defer></script></head><body class=bg-gray-100><header class="bg-blue-600 text-white p-4"><nav class="container mx-auto"><ul class="flex space-x-4"><li><a href=/ class=hover:underline>Home</a></li><li><a href=/solutions/ class=hover:underline>Solutions</a></li></ul></nav></header><main class="container mx-auto mt-8 p-4"><div class="bg-white shadow-md rounded-lg p-6"><h1 id=title class="text-3xl font-bold mb-4">Train SwinUnetr Pixel Embedding Network</h1><div class="mb-4 text-gray-600">2024-08-11</div><div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-6"><div><p><strong>Catalog:</strong> cellcanvas</p><p><strong>Group:</strong> morphospaces</p><p><strong>Name:</strong> train_swin_unetr_pixel_embedding</p><p><strong>Version:</strong> 0.0.4</p><p><strong>Authors:</strong> Kevin Yamauchi and Kyle Harrington</p><p><strong>License:</strong> MIT</p><p><strong>Tags:</strong> imaging, segmentation, cryoet, Python, morphospaces</p></div><div><div class="bg-gray-200 w-full h-48 flex items-center justify-center rounded-lg"><span class=text-gray-500>No cover image available</span></div></div></div><div class="prose max-w-none"><h2>Description</h2><p>Train the SwinUnetr pixel embedding network using the provided script and dataset.</p><h2 id=arguments>Arguments</h2><ul><li><strong>lr</strong> (float): Learning rate.</li><li><strong>logdir_path</strong> (string): Path to save logs and checkpoints.</li><li><strong>batch_size</strong> (integer): Batch size for training.</li><li><strong>patch_threshold</strong> (float): Patch threshold.</li><li><strong>loss_temperature</strong> (float): Loss temperature.</li><li><strong>pretrained_weights_path</strong> (string): Path to pretrained weights.</li><li><strong>max_epochs</strong> (integer): Maximum number of epochs for training.</li><li><strong>copick_config_path</strong> (string): Path to the Copick configuration JSON file.</li><li><strong>train_run_names</strong> (string): Comma-separated list of Copick run names for training data.</li><li><strong>val_run_names</strong> (string): Comma-separated list of Copick run names for validation data.</li><li><strong>voxel_spacing</strong> (float): Voxel spacing to be used.</li><li><strong>tomo_type</strong> (string): Type of tomogram to process.</li><li><strong>session_id</strong> (string): Session ID for accessing Copick data.</li><li><strong>user_id</strong> (string): User ID for accessing Copick data.</li><li><strong>segmentation_name</strong> (string): Name of the segmentation to use from Copick.</li></ul><h2 id=citation>Citation</h2><ul><li>Morphospaces team.
URL: <a href=https://github.com/morphometrics/morphospaces>https://github.com/morphometrics/morphospaces</a></li></ul></div></div></main><footer class="bg-gray-200 p-4 mt-8"><div class="container mx-auto text-center">&copy; 2024 Capture knowledge meta catalog</div></footer></body></html>